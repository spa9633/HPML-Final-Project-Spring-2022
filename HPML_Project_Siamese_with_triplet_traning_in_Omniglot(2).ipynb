{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HPML Project - Siamese with triplet traning in Omniglot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6BYwWeZoOen"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout, Concatenate, BatchNormalization, concatenate, ReLU, LeakyReLU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPp23RyToWWQ",
        "outputId": "549c6421-17d6-4958-8c0d-1c7d3e3013ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0Uj6kjAqFBf",
        "outputId": "51bfaa43-2de5-412e-90b6-aa9d6fa5de31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 15543541608337009761\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14465892352\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 1407240943411743949\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = '/content/drive/My Drive/HPML_Project_Siamese_Networks_kpv222_spa9633/'\n",
        "\n",
        "train_zip_path = project_path + 'images_background.zip'\n",
        "validation_zip_path = project_path + 'images_evaluation.zip'\n",
        "\n",
        "train_folder = \"/content/images_background/\"\n",
        "val_folder = '/content/images_evaluation/'\n",
        "save_path = '/content/'\n",
        "print(train_folder)\n",
        "print(val_folder)\n",
        "print(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbLVvlXXqI-o",
        "outputId": "512c8884-8ea2-4502-c22b-c0719586999a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/images_background/\n",
            "/content/images_evaluation/\n",
            "/content/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile(train_zip_path, 'r') as z:\n",
        "  z.extractall()\n",
        "print(\"Training folder zip extraction completed!\")\n",
        "\n",
        "with ZipFile(validation_zip_path, 'r') as z:\n",
        "  z.extractall()\n",
        "print(\"Validation folder zip extraction completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJYKhhbTqNMc",
        "outputId": "21b5e1aa-356f-457f-c436-5688fe9fb303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training folder zip extraction completed!\n",
            "Validation folder zip extraction completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loadimgs(path,n = 0):\n",
        "    '''\n",
        "    path => Path of train directory or test directory\n",
        "    '''\n",
        "    X=[]\n",
        "    y = []\n",
        "    cat_dict = {}\n",
        "    lang_dict = {}\n",
        "    curr_y = n\n",
        "    # we load every alphabet seperately so we can isolate them later\n",
        "    for alphabet in os.listdir(path):\n",
        "        print(\"loading alphabet: \" + alphabet)\n",
        "        lang_dict[alphabet] = [curr_y,None]\n",
        "        alphabet_path = os.path.join(path,alphabet)\n",
        "        # every letter/category has it's own column in the array, so  load seperately\n",
        "        for letter in os.listdir(alphabet_path):\n",
        "            cat_dict[curr_y] = (alphabet, letter)\n",
        "            category_images=[]\n",
        "            letter_path = os.path.join(alphabet_path, letter)\n",
        "            # read all the images in the current category\n",
        "            for filename in os.listdir(letter_path):\n",
        "                image_path = os.path.join(letter_path, filename)\n",
        "                image = imageio.imread(image_path)\n",
        "                category_images.append(image)\n",
        "                y.append(curr_y)\n",
        "            try:\n",
        "                X.append(np.stack(category_images))\n",
        "            # edge case  - last one\n",
        "            except ValueError as e:\n",
        "                print(e)\n",
        "                print(\"error - category_images:\", category_images)\n",
        "            curr_y += 1\n",
        "            lang_dict[alphabet][1] = curr_y - 1\n",
        "    y = np.vstack(y)\n",
        "    X = np.stack(X)\n",
        "    return X,y,lang_dict"
      ],
      "metadata": {
        "id": "-u4u4QV3oa0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y,c = loadimgs(train_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6LT7cnZqVRa",
        "outputId": "b5f4ebcb-285b-43b8-da81-1609c2f7ce21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading alphabet: Alphabet_of_the_Magi\n",
            "loading alphabet: Bengali\n",
            "loading alphabet: Arcadian\n",
            "loading alphabet: Japanese_(katakana)\n",
            "loading alphabet: N_Ko\n",
            "loading alphabet: Tagalog\n",
            "loading alphabet: Sanskrit\n",
            "loading alphabet: Armenian\n",
            "loading alphabet: Burmese_(Myanmar)\n",
            "loading alphabet: Early_Aramaic\n",
            "loading alphabet: Asomtavruli_(Georgian)\n",
            "loading alphabet: Anglo-Saxon_Futhorc\n",
            "loading alphabet: Hebrew\n",
            "loading alphabet: Ojibwe_(Canadian_Aboriginal_Syllabics)\n",
            "loading alphabet: Cyrillic\n",
            "loading alphabet: Korean\n",
            "loading alphabet: Braille\n",
            "loading alphabet: Japanese_(hiragana)\n",
            "loading alphabet: Gujarati\n",
            "loading alphabet: Balinese\n",
            "loading alphabet: Latin\n",
            "loading alphabet: Mkhedruli_(Georgian)\n",
            "loading alphabet: Futurama\n",
            "loading alphabet: Inuktitut_(Canadian_Aboriginal_Syllabics)\n",
            "loading alphabet: Grantha\n",
            "loading alphabet: Malay_(Jawi_-_Arabic)\n",
            "loading alphabet: Syriac_(Estrangelo)\n",
            "loading alphabet: Tifinagh\n",
            "loading alphabet: Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
            "loading alphabet: Greek\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xval,yval,cval = loadimgs(val_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSVsWMADuC33",
        "outputId": "87949f9e-26f7-4f83-d57d-90a15580ce80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading alphabet: Kannada\n",
            "loading alphabet: Aurek-Besh\n",
            "loading alphabet: Oriya\n",
            "loading alphabet: Mongolian\n",
            "loading alphabet: Gurmukhi\n",
            "loading alphabet: Manipuri\n",
            "loading alphabet: Keble\n",
            "loading alphabet: Sylheti\n",
            "loading alphabet: Tibetan\n",
            "loading alphabet: Atlantean\n",
            "loading alphabet: Old_Church_Slavonic_(Cyrillic)\n",
            "loading alphabet: Syriac_(Serto)\n",
            "loading alphabet: Tengwar\n",
            "loading alphabet: Malayalam\n",
            "loading alphabet: Angelic\n",
            "loading alphabet: Atemayar_Qelisayer\n",
            "loading alphabet: ULOG\n",
            "loading alphabet: Ge_ez\n",
            "loading alphabet: Avesta\n",
            "loading alphabet: Glagolitic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(save_path,\"train.pickle\"), \"wb\") as f:\n",
        "    pickle.dump((X,c),f)"
      ],
      "metadata": {
        "id": "8aCoE-UHtAc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(save_path,\"val.pickle\"), \"wb\") as f:\n",
        "    pickle.dump((Xval,cval),f)"
      ],
      "metadata": {
        "id": "_2Wd9WlZtHAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = save_path\n",
        "\n",
        "with open(os.path.join(PATH, \"train.pickle\"), \"rb\") as f:\n",
        "    (X_train, c_train) = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(PATH, \"val.pickle\"), \"rb\") as f:\n",
        "    (X_test, c_test) = pickle.load(f)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"\")\n",
        "print(\"training alphabets\")\n",
        "print([key for key in c_train.keys()])\n",
        "print(\"test alphabets:\")\n",
        "print([key for key in c_test.keys()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz6M4wF1tX3x",
        "outputId": "9eb9b921-26b9-4b05-a63e-e498fd3adafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (964, 20, 105, 105)\n",
            "X_test shape: (659, 20, 105, 105)\n",
            "\n",
            "training alphabets\n",
            "['Alphabet_of_the_Magi', 'Bengali', 'Arcadian', 'Japanese_(katakana)', 'N_Ko', 'Tagalog', 'Sanskrit', 'Armenian', 'Burmese_(Myanmar)', 'Early_Aramaic', 'Asomtavruli_(Georgian)', 'Anglo-Saxon_Futhorc', 'Hebrew', 'Ojibwe_(Canadian_Aboriginal_Syllabics)', 'Cyrillic', 'Korean', 'Braille', 'Japanese_(hiragana)', 'Gujarati', 'Balinese', 'Latin', 'Mkhedruli_(Georgian)', 'Futurama', 'Inuktitut_(Canadian_Aboriginal_Syllabics)', 'Grantha', 'Malay_(Jawi_-_Arabic)', 'Syriac_(Estrangelo)', 'Tifinagh', 'Blackfoot_(Canadian_Aboriginal_Syllabics)', 'Greek']\n",
            "test alphabets:\n",
            "['Kannada', 'Aurek-Besh', 'Oriya', 'Mongolian', 'Gurmukhi', 'Manipuri', 'Keble', 'Sylheti', 'Tibetan', 'Atlantean', 'Old_Church_Slavonic_(Cyrillic)', 'Syriac_(Serto)', 'Tengwar', 'Malayalam', 'Angelic', 'Atemayar_Qelisayer', 'ULOG', 'Ge_ez', 'Avesta', 'Glagolitic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Improved siamese model\n",
        "# define a convnet model to transforms data to an embeddings space. \n",
        "input_shape = (105, 105, 1)\n",
        "\n",
        "# The architecture is similar to that in the paper (Koch et al., \"Siamese Neural Networks for One-shot Image Recognition\"), \n",
        "# but we include dropout and batch normalization to improve generalization and speed up training.\n",
        "convnet = Sequential()\n",
        "convnet.add(Conv2D(64, (3,3), input_shape=input_shape))\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(ReLU())\n",
        "convnet.add(MaxPooling2D((2,2)))\n",
        "\n",
        "convnet.add(Conv2D(128, (3,3)))\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(ReLU())\n",
        "convnet.add(MaxPooling2D((2,2)))\n",
        "\n",
        "convnet.add(Conv2D(128, (3,3)))\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(ReLU())\n",
        "convnet.add(MaxPooling2D((2,2)))\n",
        "\n",
        "convnet.add(Conv2D(256, (3,3)))\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(ReLU())\n",
        "convnet.add(MaxPooling2D((2,2)))\n",
        "\n",
        "convnet.add(Flatten())\n",
        "\n",
        "convnet.add(Dense(1024, activation=\"linear\"))\n",
        "\n",
        "convnet._name = \"leg\"\n",
        "\n",
        "convnet.summary()"
      ],
      "metadata": {
        "id": "GbPoco6YofSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef12d7c-17f6-4442-e1d7-649ab763b60a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"leg\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_16 (Conv2D)          (None, 103, 103, 64)      640       \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 103, 103, 64)     256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_16 (ReLU)             (None, 103, 103, 64)      0         \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  (None, 51, 51, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 49, 49, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 49, 49, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_17 (ReLU)             (None, 49, 49, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPoolin  (None, 24, 24, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 22, 22, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 22, 22, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_18 (ReLU)             (None, 22, 22, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPoolin  (None, 11, 11, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 9, 9, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 9, 9, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_19 (ReLU)             (None, 9, 9, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPoolin  (None, 4, 4, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1024)              4195328   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,714,880\n",
            "Trainable params: 4,713,728\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The anchor, positive, negative image are merged together, as the input of the triplet network, then got split to get each one's neural codes.\n",
        "generated = Input(shape=(3, 105, 105, 1), name='input')\n",
        "\n",
        "anchor = Lambda(lambda x: x[:, 0])(generated)\n",
        "pos = Lambda(lambda x: x[:, 1])(generated)\n",
        "neg = Lambda(lambda x: x[:, 2])(generated)\n",
        "\n",
        "# merge the anchor, positive, negative embedding together, \n",
        "# let the merged layer be the output of triplet network\n",
        "anchor_embedding = convnet(anchor)\n",
        "pos_embedding = convnet(pos)\n",
        "neg_embedding = convnet(neg)  \n",
        "\n",
        "merged_output = concatenate([anchor_embedding, pos_embedding, neg_embedding], axis=-1, name='merged_layer')\n",
        "\n",
        "triplet_net = Model(inputs=generated, outputs=merged_output)\n",
        "triplet_net.summary()"
      ],
      "metadata": {
        "id": "BNg7kWX-oiUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dca4b08-cae8-48a0-c079-53456965fe4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_100\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 3, 105, 105  0           []                               \n",
            "                                , 1)]                                                             \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 105, 105, 1)  0           ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 105, 105, 1)  0           ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 105, 105, 1)  0           ['input[0][0]']                  \n",
            "                                                                                                  \n",
            " leg (Sequential)               (None, 1024)         4714880     ['lambda_12[0][0]',              \n",
            "                                                                  'lambda_13[0][0]',              \n",
            "                                                                  'lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " merged_layer (Concatenate)     (None, 3072)         0           ['leg[0][0]',                    \n",
            "                                                                  'leg[1][0]',                    \n",
            "                                                                  'leg[2][0]']                    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,714,880\n",
            "Trainable params: 4,713,728\n",
            "Non-trainable params: 1,152\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice that the ground truth variable is not used for loss calculation. \n",
        "# It is used as a function argument to by-pass some Keras functionality.\n",
        "# This is because the network structure already implies the ground truth for the anchor image with the \"positive\" image.\n",
        "def triplet_loss(ground_truth, network_output):\n",
        "\n",
        "    anchor, positive, negative = tf.split(network_output, num_or_size_splits=3, axis=1)        \n",
        "\n",
        "    # This is an easy implementation, but also a very inefficient one because it uses offline triplet mining (https://omoindrot.github.io/triplet-loss)\n",
        "    positive_distance = tf.reduce_sum(tf.square(anchor - positive), 1)\n",
        "    negative_distance = tf.reduce_sum(tf.square(anchor - negative), 1)\n",
        "\n",
        "    margin = 2000\n",
        "    loss = tf.maximum(positive_distance - negative_distance + margin, 0.0)\n",
        "    loss = tf.reduce_mean(loss)\n",
        " \n",
        "    return loss"
      ],
      "metadata": {
        "id": "kJKKYOMAovgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice that the returned  1 * np.zeros(batch_size) is to by-pass some Keras functionality, corresponding to ground_truth in tripletloss\n",
        "# We use a variable hard_selection to control which method we are going to use. If we set hard_selection == False, we will select triplets random,If we set the variable hard_selection == True, we will select hard triplets.\n",
        "def get_batch(batch_size, X, hard_selection):\n",
        "    # Create a subset of the model that basically represents a \"leg\" of the model\n",
        "    subset_model = Model(inputs=triplet_net.get_layer(\"leg\").get_input_at(0), \n",
        "                         outputs=triplet_net.get_layer(\"leg\").get_output_at(0))\n",
        "\n",
        "    while True:\n",
        "        n_classes, n_examples, w, h = X.shape\n",
        "        \n",
        "        # initialize result\n",
        "        triplets = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            triplet = [[], [], []]\n",
        "\n",
        "            # Pick one random class for anchor\n",
        "            anchor_class = np.random.randint(0, n_classes)\n",
        "\n",
        "            # Pick two different random pics for this class => idx_A and idx_P\n",
        "            [idx_A, idx_P] = np.random.choice(n_examples, size=2, replace=False)\n",
        "            #print(f\"Anchor class: {anchor_class}, idx_A: {idx_A}, idx_P: {idx_P}\")\n",
        "            \n",
        "            # Pick another class for negative, different from anchor_class\n",
        "            negative_class = np.random.choice(np.setdiff1d(range(0, n_classes), anchor_class))\n",
        "            # print(f\"Negative class: {negative_class}, shape: {X[negative_class].shape}\")\n",
        "\n",
        "            if not hard_selection:\n",
        "                # Pick a random pic from this negative class => N \n",
        "                idx_N = np.random.choice(n_examples, size=1, replace=False)\n",
        "\n",
        "            else:\n",
        "                # Pick a hardest pic from this negative class => N\n",
        "                \n",
        "                # Get the embedding of the anchor image\n",
        "                anchor_img = subset_model.predict(np.expand_dims(X[anchor_class][idx_A], axis=0))\n",
        "\n",
        "                # Make a prediction for all images in the negative class\n",
        "                neg_imgs = subset_model.predict(np.expand_dims(X[negative_class], axis=0).reshape(20, 105, 105, 1))\n",
        "                \n",
        "                # Compute the distance (note that we use the l2 distance) between the anchor and negative img embeddings\n",
        "                distances = [np.linalg.norm(anchor_img - neg_img) for neg_img in neg_imgs]\n",
        "\n",
        "                # Pick the image with the nearest distance as the \"hard\" image\n",
        "                idx_N = np.argsort(distances)[0]\n",
        "\n",
        "            triplet[0] = X[anchor_class][idx_A].reshape(w, h, 1)\n",
        "            triplet[1] = X[anchor_class][idx_P].reshape(w, h, 1)\n",
        "            triplet[2]=  X[negative_class][idx_N].reshape(w, h, 1)\n",
        "            triplets.append(triplet)\n",
        "\n",
        "        yield np.array(triplets), 1 * np.zeros(batch_size)"
      ],
      "metadata": {
        "id": "jFv8CM7ko1P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_oneshot_task(N, X, c, language=None):\n",
        "    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n",
        "    n_classes, n_examples, w, h = X.shape\n",
        "    indices = np.random.randint(0, n_examples, size=(N,))\n",
        "    if language is not None:\n",
        "        low, high = c[language]\n",
        "        if N > high - low:\n",
        "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
        "        categories = np.random.choice(range(low,high), size=(N,), replace=False)\n",
        "    else:  # if no language specified just pick a bunch of random letters\n",
        "        categories = np.random.choice(range(n_classes), size=(N,), replace=False)            \n",
        "    true_category = categories[0]\n",
        "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
        "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, 1)\n",
        "    support_set = X[categories, indices, :, :]\n",
        "    support_set[0, :, :] = X[true_category, ex2]\n",
        "    support_set = support_set.reshape(N, w, h, 1)\n",
        "    targets = np.zeros((N,))\n",
        "    targets[0] = 1\n",
        "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
        "    pairs = [test_image, support_set]\n",
        "    return pairs, targets"
      ],
      "metadata": {
        "id": "7AWx8Gfzo4g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_oneshot(model, X, c, N=20, k=250, language=None, verbose=True):     \n",
        "    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n",
        "    n_correct = 0\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n",
        "\n",
        "    for i in range(k):\n",
        "        # Create a one-shot task \n",
        "        inputs, targets = make_oneshot_task(N, X, c, language=language)\n",
        "\n",
        "        # 1. For a given one-shot task, obtain embeddings for the test image as well as the support set. \n",
        "        test_img = model.predict(inputs[0])\n",
        "        support_set = model.predict(inputs[1])\n",
        "        # Note that we use the l2 distance to compute the distances\n",
        "        distances = [np.linalg.norm(x-y) for x,y in zip(test_img, support_set)]\n",
        "        \n",
        "        # 2. Pick the image from the support set that is closest (in L2-distance) to the test image as your one-shot prediction.\n",
        "        if np.argmin(distances) == np.argmax(targets):\n",
        "            n_correct += 1\n",
        "\n",
        "    percent_correct = (100.0 * n_correct / k)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(percent_correct, N))\n",
        "    return percent_correct"
      ],
      "metadata": {
        "id": "-BAWT1iNo-Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, X_train, hard_selection=False, batch_size=64, steps_per_epoch=100, epochs=1):\n",
        "    model.fit(get_batch(batch_size, X_train, hard_selection), steps_per_epoch=steps_per_epoch, epochs=epochs)"
      ],
      "metadata": {
        "id": "U4ruXvU_pDdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random triplet selection\n",
        "triplet_net.compile(loss=triplet_loss, optimizer=Adam(lr=0.0001))\n",
        "loops = 20\n",
        "best_acc_random = 0\n",
        "t1 = time.time()\n",
        "for i in range(loops):\n",
        "    print(\"=== Training loop {} ===\".format(i+1))\n",
        "    # === ADD CODE HERE ===\n",
        "    train(triplet_net, X_train, hard_selection=False, batch_size=64, steps_per_epoch=100, epochs=1)\n",
        "    subset_model = Model(inputs=triplet_net.get_layer(\"leg\").get_input_at(0), \n",
        "                         outputs=triplet_net.get_layer(\"leg\").get_output_at(0))\n",
        "    test_acc = test_oneshot(subset_model, X_test, c_test)\n",
        "\n",
        "    if test_acc >= best_acc_random:\n",
        "        print(\"********* New best one-shot accuracy, saving model ********\")\n",
        "        triplet_net.save(os.path.join(\".\", \"triplet_net_with_random_selection.h5\"))\n",
        "        best_acc_random = test_acc\n",
        "\n",
        "    if test_acc >= 80:\n",
        "      t2 = time.time()\n",
        "      print(\"The time taken to reach the TTA target of 80% is\", (t2-t1)/60, \"minutes\")\n",
        "      sys.exit(0)"
      ],
      "metadata": {
        "id": "Mn_uvB-PpG1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a872a7-1794-4993-90af-107d76dc1eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training loop 1 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 18s 167ms/step - loss: 568.8652\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 58.0% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 2 ===\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 265.8008\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 68.0% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 3 ===\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 192.9664\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 71.2% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 4 ===\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 155.3952\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 72.4% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 5 ===\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 144.4606\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 71.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 6 ===\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 118.5761\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 73.6% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 7 ===\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 124.7000\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 74.0% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 8 ===\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 120.4038\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 72.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 9 ===\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 110.2721\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 71.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 10 ===\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 109.0425\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 70.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 11 ===\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 102.2967\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 77.2% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 12 ===\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 103.0594\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 82.4% accuracy for 20-way one-shot learning\n",
            "********* New best one-shot accuracy, saving model ********\n",
            "=== Training loop 13 ===\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 93.9072\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 76.0% accuracy for 20-way one-shot learning\n",
            "=== Training loop 14 ===\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 74.8926\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 77.2% accuracy for 20-way one-shot learning\n",
            "=== Training loop 15 ===\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 81.3240\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 76.4% accuracy for 20-way one-shot learning\n",
            "=== Training loop 16 ===\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 76.7236\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 73.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 17 ===\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 64.8684\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 76.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 18 ===\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 70.4715\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 81.6% accuracy for 20-way one-shot learning\n",
            "=== Training loop 19 ===\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 61.5642\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 78.8% accuracy for 20-way one-shot learning\n",
            "=== Training loop 20 ===\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 64.0607\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 74.0% accuracy for 20-way one-shot learning\n"
          ]
        }
      ]
    }
  ]
}